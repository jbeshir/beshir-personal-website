---
layout: ../../layouts/summary.astro
title: "A Whirlwind Tour of LW Rationality: 6&nbsp;Books in 32 Pages - Mere Goodness"
date: "2016-07-09"
categories: 
  - "rationality"
---

[(Back to "Mere Reality")](/summaries/a-whirlwind-tour-of-lw-rationality-mere-reality)

### Mere Goodness U: Fake Preferences

Human desires include preferences for how the world is, not just preferences for how they think the world is or how happy they are. ([Not For The Sake Of Happiness Alone](http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/)) People who claim their preferences reduce down to a single principle have some other process by which they choose what they want, and then find a rationalisation for how what they want is justified by that principle ([Fake Selfishness](http://lesswrong.com/lw/kx/fake_selfishness/)). Simple utility functions fail to compress our values, and we suffer from anthropomorphic optimism about what they suggest. ([Fake Utility Functions](http://lesswrong.com/lw/lq/fake_utility_functions/))

People who fear that humans would lack morality without an external threat, regard this as bad rather than liberating. This means they like morality, and aren’t just forced to abide by it. ([Fake Morality](http://lesswrong.com/lw/ky/fake_morality/))

The _detached lever fallacy_ is the assumption that actions that trigger behaviour from one entity will trigger it from another, without any reason to think the mechanics governing the reaction are present in the second. The actions that make a human compassionate will not make a non-human AI so. ([Detached Lever Fallacy](http://lesswrong.com/lw/sp/detached_lever_fallacy/)) AI design is reducing the mental to the non-mental. Models of an intelligence which can’t predict what it will do other than by analogy to a human are incomplete. ([Dreams Of AI Design](http://lesswrong.com/lw/tf/dreams_of_ai_design/)) The space of possible minds is extremely large. Resist the temptation to generalise over all of mind design space. ([The Design Space Of Minds-In-General](http://lesswrong.com/lw/rm/the_design_space_of_mindsingeneral/))

### Mere Goodness V: Value Theory

Justifying any belief leads to infinite regress. Rather than accepting any assumption, we should reflect on our mind’s trustworthiness using our current mind as best we can, and accept that. ([Where Recursive Justification Hits Bottom](http://lesswrong.com/lw/s0/where_recursive_justification_hits_bottom/)) Approach such questions from the standpoint of whether we should want ourselves or an AI using similar principles to change how they choose beliefs. We should focus on improvement, not justification, and expect to change our minds. Don’t exalt consistency in itself, but effectiveness. Separate asking “why” an approach works from whether it “does”. We should reason about our own mind the way we do about the rest of the world, and use all available information. ([My Kind Of Reflection](http://lesswrong.com/lw/s2/my_kind_of_reflection/))

There are no arguments compelling to all possible minds. For any system processing information, there is a system with inverted output which makes the opposite conclusion. This applies to moral conclusions, and regardless of the intelligence of the system. ([No Universally Compelling Arguments](http://lesswrong.com/lw/rn/no_universally_compelling_arguments/), [Sorting Pebbles Into Correct Heaps](http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/)) A mind must have a process that adds beliefs, and a process that acts, or no argument can convince it to believe or act. ([Created Already In Motion](http://lesswrong.com/lw/rs/created_already_in_motion/))

Some properties can be either thought of as as taking two parameters and giving a result, or as a space of one-parameter functions, with different people using different ones. For example, ‘attractiveness(admirer, admired) -> result’ vs ‘attractiveness\__1...9999_(admired) -> result’. _Currying_ specifies that a two parameter function is equivalent to a one parameter function returning another function, and unifies these. For example, ‘attractiveness(admirer) -> attractiveness\_712(admired) -> result’. This reflects the ability to judge a measure independently of the user, but also that the measure used is variable. ([2-Place And 1-Place Words](http://lesswrong.com/lw/ro/2place_and_1place_words/))

If your moral framework is shown to be invalid, you can still choose to act morally anyway. ([What Would You Do Without Morality?](http://lesswrong.com/lw/rq/what_would_you_do_without_morality/)) It’s important to have a line of retreat to be able to seriously review your metaethics. ([Changing Your Metaethics](http://lesswrong.com/lw/sk/changing_your_metaethics/)) You must start from a willingness to evaluate in terms of your moral intuition in order to find valid metaethics. ([Could Anything Be Right?](http://lesswrong.com/lw/sb/could_anything_be_right/)) What we consider to be right grows out of a starting point. To get a system that specifies what is right requires it fit that starting point, which we cannot define fully. ([Morality As Fixed Computation](http://lesswrong.com/lw/sw/morality_as_fixed_computation/)) Concepts that we develop to describe good behaviour are very complex. Depictions of them have many possible concepts that fit them, and an algorithm would pick the wrong one.You cannot fix a powerful optimisation process optimising for the wrong thing with patches. ([Magical Categories](http://lesswrong.com/lw/td/magical_categories/)) Value is fragile; optimising for the wrong values creates a dull future. ([Value Is Fragile](http://lesswrong.com/lw/y3/value_is_fragile/)) Our complicated values are the gift that we give to tomorrow. ([The Gift We Give To Tomorrow](http://lesswrong.com/lw/sa/the_gift_we_give_to_tomorrow/))

The _prisoner’s dilemma_ is a hypothetical in which two people can both either cooperate (C) or defect (D), and each one prefers (D, C) > (C, C) > (D, D) > (C, D). The typical example involves two totally selfish prisoners, but humans can’t imagine this. A better example would have the first entity as humans trying to save billions, vs an entity trying to maximise numbers of paperclips. ([The True Prisoner's Dilemma](http://lesswrong.com/lw/tn/the_true_prisoners_dilemma/))

We understand others by simulating them with our brains, which creates empathy. It was evolutionarily useful to develop sympathy. An AI wouldn’t use either approach, an alien might. ([Sympathetic Minds](http://lesswrong.com/lw/xs/sympathetic_minds/))

A world with no difficulty would be boring, We prefer real goals to fake ones. We need goals which we prefer working on to having finished, or which have no end state. ([High Challenge](http://lesswrong.com/lw/ww/high_challenge/)) A utopia with no problems has no stories. Pain can be more intense than pleasure. Pleasure that scaled like pain would trap us. We can be rid of pain that breaks or grinds down people, and pointless sorrow, and keep what we value. Whether we will get rid of pain entirely someday, EY does not know. ([Serious Stories](http://lesswrong.com/lw/xi/serious_stories/))

### Mere Goodness W: Quantified Humanism

_Scope insensitivity_ is ignoring the number of people or animals or area affected, the _scope_, when deciding how important an action is. Groups were asked how much they would pay to save 2000 / 20000 / 200000 migrating birds from drowning in oil ponds, and answered $80, $78, and $88. We visualise a single bird, react emotionally, and cannot visualise scope. To be an effective altruist, we must evaluate the numbers. ([Scope Insensitivity](http://lesswrong.com/lw/hw/scope_insensitivity/)) Saving one life feels as good as many, but is not as good. We do not treat saving lives as a satisficed virtue, such that once you’ve saved one you ignore others. ([One Life Against The World](http://lesswrong.com/lw/hx/one_life_against_the_world/))

The _certainty effect_ is a bias where going from 99% chance to near 100% chance of getting what we want is valued more than going from, say, 33% to 34%. This causes the _allais paradox_, where we prefer a fixed prize over a 33/34 chance of a bigger prize, but prefer a 33% chance of a larger prize to a 34% chance of a smaller prize. This cannot be explained by non-linear marginal utility of money, permits extracting money from you, and shows a failure of intuition to steer reality. ([The Allais Paradox](http://lesswrong.com/lw/my/the_allais_paradox/), [Zut Allais!](http://lesswrong.com/lw/mz/zut_allais/))

A certain loss feels worse than an uncertain one. By changing the point of comparison so the certain outcome is a loss rather than a gain, you reverse intuition. You must multiply out costs and benefits, or you will fail at directing reality. This reduces nice feelings, but they are not the point. ([Feeling Moral](https://wiki.lesswrong.com/wiki/Feeling_Moral))

Intuition is what morality is built on, but we must pursue reflective intuitions or we won’t accomplish anything due to circular preferences. ([The Intuitions Behind Utilitarianism](http://lesswrong.com/lw/n9/the_intuitions_behind_utilitarianism/)) Making up probabilities can trick you into thinking they’re more grounded than they are, and override working intuitions. ([When (Not) To Use Probabilities](http://lesswrong.com/lw/sg/when_not_to_use_probabilities/))

_Ends don’t justify the means among humans_. We run on _corrupted hardware_; we rationalise using bad means, past the point that benefits us, let alone anyone else. Otherwise we wouldn’t have developed _ethical injunctions_. Follow them as a higher-level consequentialist strategy. ([Ends Don't Justify Means Among Humans](http://lesswrong.com/lw/uv/ends_dont_justify_means_among_humans/), [Ethical Injunctions](http://lesswrong.com/lw/v1/ethical_injunctions/))

To pursue rationality effectively, you must have a higher goal that it serves. ([Something To Protect](http://lesswrong.com/lw/nb/something_to_protect/)) _Newcomb’s problem_ is a scenario in which an entity that can predict you perfectly offers two boxes, and says that box A contains $1000, and box B contains $1,000,000 if and only if they predicted you would only take box B. Traditional causal decision theory says you should take both boxes, as the money is either already in the box or not. Rationally, you should take only box B. Doing so makes you win more, and _rationality is about winning_, not about reasonableness or any particular ritual of thought. ([Newcomb's Problem And Regret Of Rationality](http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/))

[(Continue with "Becoming Stronger")](/summaries/a-whirlwind-tour-of-lw-rationality-becoming-stronger)
