---
layout: ../../layouts/summary.astro
title: "A Whirlwind Tour of LW Rationality: 6&nbsp;Books in 32 Pages - How To Actually Change Your Mind"
subpost: "true"
date: "2016-07-09"
categories: 
  - "rationality"
---

[(Back to "Map And Territory")](/summaries/a-whirlwind-tour-of-lw-rationality-map-and-territory)

### How To Actually Change Your Mind E: Overly Convenient Excuses

Humility is a complicated virtue, and we should judge it by whether applying it makes us stronger or weaker, and by whether it is an excuse to shrug. To be correctly humble is to take action in anticipation of one’s own errors. ([The Proper Use Of Humility](http://lesswrong.com/lw/gq/the_proper_use_of_humility/))

A _package deal fallacy_ is where you assume things traditionally grouped together must always be so. A _false dilemma_ is presenting only two options where more exist. Justifications for noble lies are usually one of the two; it is preferable to seek a third alternative, which may be less convenient. ([The Third Alternative](http://lesswrong.com/lw/hu/the_third_alternative/))

Human hope is limited and valuable, and the likes of lotteries waste it. ([Lotteries: A Waste Of Hope](http://lesswrong.com/lw/hl/lotteries_a_waste_of_hope/), [New Improved Lottery](http://lesswrong.com/lw/hm/new_improved_lottery/)) There is a bias in which extremely tiny chances are treated as more than tiny in implication, and justify proclaiming belief in them. There is a tendency to arbitrarily choose to ‘believe’ or not believe a thing rather than reacting to probabilities. ([But There's Still A Chance, Right?](http://lesswrong.com/lw/ml/but_theres_still_a_chance_right/))

The _fallacy of grey_ is to regard all imperfection and all uncertainty as equal. Wrong is relative. ([The Fallacy Of Grey](http://lesswrong.com/lw/mm/the_fallacy_of_gray/)) There is a sizeable inferential distance from thinking of knowledge as absolutely true to understanding knowledge as probabilistic. ([Absolute Authority](http://lesswrong.com/lw/mn/absolute_authority/)) Eliezer says he would be convinced that 2 + 2 = 3 by the same processes that convinced him that 2 + 2 = 4; a combination of physical observation, mental visualization, and social agreement, such as observing that putting two more objects down beside two objects produced three objects. ([How To Convince Me That 2+2=3](http://lesswrong.com/lw/jr/how_to_convince_me_that_2_2_3/))

Because of how evidence works, a probability of 100% or 0% corresponds to infinite certainty, and requires infinite evidence to correctly attain. As a result it is always incorrect. ([Infinite Certainty](http://lesswrong.com/lw/mo/infinite_certainty/)) 0 and 1 are \[in a sense\] not probabilities. ([0 And 1 Are Not Probabilities](http://lesswrong.com/lw/mp/0_and_1_are_not_probabilities/))

It is reasonable to care how other humans think, as part of caring about how the future and present look. This is somewhat dangerous, and so must be tempered by a solid commitment to respond to bad thinking only with argument. ([Your Rationality Is My Business](http://lesswrong.com/lw/hn/your_rationality_is_my_business/))

### How To Actually Change Your Mind F: Politics and Rationality

_Politics is the mind-killer_. People cannot think clearly about politics close to them. In politics, _arguments are soldiers_. When giving examples, it is tempting to use contemporary politics. Avoid this if possible. If you are discussing something innately political, use an example from historic politics with minimal contemporary implications if possible. ([Politics Is The Mind-Killer](http://lesswrong.com/lw/gw/politics_is_the_mindkiller/))

_Policy debates should not appear one-sided_. Actions with many consequences should not be expected to have exclusively positive or negative consequences. If they appear to, this is normally the result of bias. They may legitimately have lopsided costs and benefits. ([Policy Debates Should Not Appear One-Sided](http://lesswrong.com/lw/gz/policy_debates_should_not_appear_onesided/))

Humans tend to treat debates as a contest between two sides, where any weakness in one side is a gain to the other and visa versa, and whoever wins is correct on everything and whoever loses is wrong on everything. This is correct behaviour for a single, strictly binary question, but an error for any more complicated debate. ([The Scales Of Justice, The Notebook Of Rationality](http://lesswrong.com/lw/h1/the_scales_of_justice_the_notebook_of_rationality/))

The _fundamental attribution error_ is a tendency in people to overly attribute the actions of others to innate traits, while overly attributing their own actions to circumstance as opposed to differences in themselves. Most people see themselves as normal. ([Correspondence Bias](http://lesswrong.com/lw/hz/correspondence_bias/)) Even your worst enemies are not innately evil, and usually view themselves as the heroes of their own story. ([Are Your Enemies Innately Evil?](http://lesswrong.com/lw/i0/are_your_enemies_innately_evil/))

Stupidity causes more random beliefs, not reliably wrong ones, so reversing the beliefs of the foolish does not create correct beliefs; _reversed stupidity is not intelligence_. Foolish people disagreeing does not mean that you are correct. ([Reversed Stupidity Is Not Intelligence](http://lesswrong.com/lw/lw/reversed_stupidity_is_not_intelligence/))

Authority can be a useful guide to truth before you’ve heard arguments, but is not so after arguments. ([Argument Screens Off Authority](http://lesswrong.com/lw/lx/argument_screens_off_authority/)) The more distant from the specific question evidence is, the weaker it is. You should try to answer questions using direct evidence- _hug the query_. Otherwise learning abstract arguments, including about biases, can make you less rather than more accurate. ([Hug The Query](http://lesswrong.com/lw/ly/hug_the_query/))

Speakers may manipulate their phrasing to alter what aspects of a situation are noticed. ([Rationality And The English Language](http://lesswrong.com/lw/jc/rationality_and_the_english_language/)) Simplifying language interferes with this, and allows you to recognise errors in your own speech. ([Human Evil And Muddled Thinking](http://lesswrong.com/lw/jd/human_evil_and_muddled_thinking/))

### How To Actually Change Your Mind G: Against Rationalization

Because humans are irrational to start with, more knowledge can hurt you. Knowledge of biases gives you ammunition to use against arguments, including knowledge of this one. ([Knowing About Biases Can Hurt People](http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/))

Expect occasional opposing evidence for any imperfectly exact model. You should not look for reasons to reject it, but _update incrementally_ as it suggests. If your model is good, you will see evidence supporting it soon. ([Update Yourself Incrementally](http://lesswrong.com/lw/ij/update_yourself_incrementally/)) You should not decide what direction to change your opinion in by comparing new evidence to old arguments; this double-counts evidence. ([One Argument Against An Army](http://lesswrong.com/lw/ik/one_argument_against_an_army/))

The sophistication with which you construct arguments does not improve your conclusions; that requires choosing what to argue in a manner that entangles your choice with the truth. ([The Bottom Line](http://lesswrong.com/lw/js/the_bottom_line/))

Reaction to evidence that someone is filtering must include reacting to knowledge of the filtering. Knowing what is true can require looking at evidence from multiple parties. ([What Evidence Filtered Evidence?](http://lesswrong.com/lw/jt/what_evidence_filtered_evidence/))

_Rationalization_ is determining your reasoning after your conclusion, and runs in the opposite direction to rationality. ([Rationalization](http://lesswrong.com/lw/ju/rationalization/)) You cannot create a rational argument this way, whatever you cite. ([A Rational Argument](http://lesswrong.com/lw/jw/a_rational_argument/))

Humans tend to consider only the critiques of their position that they know they can defeat. ([Avoiding Your Belief's Real Weak Points](http://lesswrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/)) A _motivated skeptic_ asks if the evidence compels them to believe; a _motivated credulist_ asks if the evidence allows them to believe. _Motivated stopping_ is ceasing the search for opposing evidence earlier when you agree, and _motivated continuation_ is searching longer when you don’t. ([Motivated Stopping And Motivated Continuation](http://lesswrong.com/lw/km/motivated_stopping_and_motivated_continuation/))

_Fake justification_ is searching for a justification for a belief which is not the one which led you to originally hold it. ([Fake Justification](http://lesswrong.com/lw/kq/fake_justification/)) Justifications for rejecting a proposition are often not the person’s _true objection_, which when dispelled would result in the proposition being accepted. ([Is That Your True Rejection?](http://lesswrong.com/lw/wj/is_that_your_true_rejection/))

Facts about reality are often entangled with each other. ([Entangled Truths, Contagious Lies](http://lesswrong.com/lw/uw/entangled_truths_contagious_lies/), [Of Lies And Black Swan Blowups](http://lesswrong.com/lw/9a/of_lies_and_black_swan_blowups/)) Maintaining a false belief often requires other false beliefs, including deception about evidence and rationality themselves. ([Dark Side Epistemology](http://lesswrong.com/lw/uy/dark_side_epistemology/))

### How To Actually Change Your Mind H: Against Doublethink

In _doublethink_, you forget then forget you have forgotten. In _singlethink_, you notice yourself forgetting an uncomfortable thought and recall it. ([Singlethink](http://lesswrong.com/lw/k0/singlethink/))

If you watch the risks of doublethink enough to do it only when useful, you cannot do it. If you do not, you will do it where it harms you. Doublethink is either not an option or harmful. ([Doublethink (Choosing To Be Biased)](http://lesswrong.com/lw/je/doublethink_choosing_to_be_biased/))

The above on doublethink not be a dispassionate reporting of the facts; Eliezer admits that they may have been tempted into trying to create a self-fulfilling prophecy. They then say that it may be wise to at least tell yourself that you can’t self-deceive, so that you aren’t tempted to try. ([Don't Believe You'll Self-Deceive](http://lesswrong.com/lw/1o/dont_believe_youll_selfdeceive/))

It is possible to lead yourself to think you believe something without believing it. Believing that a belief is good can lead you to false belief-in-belief. ([No, Really, I've Deceived Myself](http://lesswrong.com/lw/r/no_really_ive_deceived_myself/), [Belief In Self-Deception](http://lesswrong.com/lw/s/belief_in_selfdeception/)) We often do not separate believing a belief from endorsing a belief. Belief-in-belief can create apparently contradictory beliefs. ([Moore's Paradox](http://lesswrong.com/lw/1f/moores_paradox/))

### How To Actually Change Your Mind I: Seeing With Fresh Eyes

_Anchoring_ is a behaviour in which we take a figure we’ve recently seen and adjust it to answer questions, making results depend on the initial anchor. A strategy for countering it might be to dwell on an alternative anchor if you notice an initial guess is implausible. ([Anchoring And Adjustment](http://lesswrong.com/lw/j7/anchoring_and_adjustment/))

_Priming_ is an aspect of our brain’s architecture. Concepts related to ideas we’ve recently had in mind are recalled faster. This means that completely irrelevant observations influence estimates and decisions. This is known as _contamination_. It supports _confirmation bias_; having an idea in our head makes compatible ideas come to mind more easily, making us more receptive to confirming than disconfirming evidence for our beliefs. ([Priming And Contamination](http://lesswrong.com/lw/k3/priming_and_contamination/))

Some evidence suggests that we tend to initially believe statements, then adjust to reject false ones. Being distracted makes us more likely to believe statements explicitly labeled as false. ([Do We Believe Everything We're Told?](http://lesswrong.com/lw/k4/do_we_believe_everything_were_told/))

The _hundred-step rule_ is the principle that because neurons in the human brain are slow, any hypothesised operation can be very parallel but must complete in under a hundred sequential neuron spikes. It is a good guess that human cognition consists mostly of cache lookups.

We incorporate the thoughts of others into this cache, and alone could not regenerate all the ideas we’ve collected in a single lifetime. We tend to incorporate and then repeat or act on _cached thoughts_ without thinking about their source or credibility. ([Cached Thoughts](http://lesswrong.com/lw/k5/cached_thoughts/))

“Outside the box” thinking is a box of its own, and along with stated efforts at originality and subversive thinking follows predictable patterns; genuine originality requires thinking. ([The "Outside The Box" Box](http://lesswrong.com/lw/k6/the_outside_the_box_box/)) When a topic seems to have nothing to be said, it can mean we do not have any related cached thoughts, and find generating new ones difficult. ([Original Seeing](http://lesswrong.com/lw/k7/original_seeing/))

The events of history would sound extremely strange described to someone prior to them. ([Stranger Than History](http://lesswrong.com/lw/j1/stranger_than_history/)) We tend to treat fiction as history which happened elsewhere. This causes us to favour hypotheses which fit into fun narratives, over other hypotheses that might be likely. ([The Logical Fallacy Of Generalization From Fictional Evidence](http://lesswrong.com/lw/k9/the_logical_fallacy_of_generalization_from/))

A model which connects all things contains the same information as a model that connects none. Information is contained in selectiveness about connections, and the more fine-grained this is the more information is contained. The _virtue of narrowness_ is the definition and use of narrow terms and ideas rather than broad ones. ([The Virtue Of Narrowness](http://lesswrong.com/lw/ic/the_virtue_of_narrowness/))

One may sound deep by coherently expressing cached thoughts that the listener hasn’t heard yet. One may be deep by attempting to see for yourself rather than following standard patterns. ([How To Seem And Be Deep](http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/))

We change our mind less often than we think, and are resistant to it. A technique to mitigate against this is to _hold off on proposing solutions_ as long as possible. ([We Change Our Minds Less Often Than We Think](http://lesswrong.com/lw/jx/we_change_our_minds_less_often_than_we_think/), [Hold Off On Proposing Solutions](http://lesswrong.com/lw/ka/hold_off_on_proposing_solutions/))

Because of confirmation bias, we should be suspicious of ideas that originally came from sources whose output was not entangled with the truth. However, to disregard other evidence entirely in favour of judging the original source would be the _genetic fallacy_. ([The Genetic Fallacy](http://lesswrong.com/lw/s3/the_genetic_fallacy/))

### How To Actually Change Your Mind J: Death Spirals and the Cult Attractor

The _affect heuristic_ is when subjective impressions of goodness/badness act as a heuristic. It causes the manner in which a problem is stated and irrelevant aspects of a situation to change the decisions we make. ([The Affect Heuristic](http://lesswrong.com/lw/lg/the_affect_heuristic/)) The _halo effect_ is this applied to people; when our subjective impression of a person in one regard, such as appearance, alters our judgement of them in others. ([The Halo Effect](http://lesswrong.com/lw/lj/the_halo_effect/))

We overestimate the altruism of those who run less risk compared to those who run more, and attribute less virtue to people who are generous for lesser as well as greater need. ([Superhero Bias](http://lesswrong.com/lw/lk/superhero_bias/)) We lionize messiahs for whom doing great things is easy over those for whom it is hard. ([Mere Messiahs](http://lesswrong.com/lw/ll/mere_messiahs/))

We tend to evaluate things against nearby points of comparison. ([Evaluability And Cheap Holiday Shopping](http://lesswrong.com/lw/lh/evaluability_and_cheap_holiday_shopping/)) When we lack a bounded scale to put our estimates within, we make one up, inconsistently between people. ([Unbounded Scales, Huge Jury Awards, And Futurism](http://lesswrong.com/lw/li/unbounded_scales_huge_jury_awards_futurism/))

An _affective death spiral_ is a scenario in which a strong positive impression assigned to one idea causes us to improve our impressions of related ideas, which we then treat as confirmation of the original idea in a self-sustaining cycle. ([Affective Death Spirals](http://lesswrong.com/lw/lm/affective_death_spirals/)) We can diminish the effect of positive impressions enough to prevent this by splitting big ideas into smaller ones we treat independently, reminding ourselves of the conjunctive bias and considering each additional claim to be a burdensome detail, and following the suggestions in the Against Rationalization sequence. ([Resist The Happy Death Spiral](http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/))

Considering it morally wrong to criticise an idea accelerates an affective death spiral. ([Uncritical Supercriticality](http://lesswrong.com/lw/lo/uncritical_supercriticality/)) _Evaporative cooling of group beliefs_ is a scenario in which as a group becomes more extreme, moderates leave, and as they are no longer acting as a brake, the group becomes yet more extreme, in a cycle. This is another reason why tolerating dissent is important. ([Evaporative Cooling Of Group Beliefs](http://lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/))

A _spiral of hate_ is the mirror image of an affective death spiral, in which a strong negative impression of a thing causes us to believe related negative ideas, which we then treat as strengthening the original impression. You can correspondingly observe it become morally wrong to urge restraint or to object to a criticism. It, too, leads to poor choice of action. ([When None Dare Urge Restraint](http://lesswrong.com/lw/ls/when_none_dare_urge_restraint/))

Humans, once divided into opposing groups, will naturally form positive and negative stereotypes of the two groups and engage in conflict. ([The Robbers Cave Experiment](http://lesswrong.com/lw/lt/the_robbers_cave_experiment/)) Every cause has a natural tendency for its supporters to become focused on defending their group, even if they declare ‘rationality’ to be their goal. ([Every Cause Wants To Be A Cult](http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/))

Beware being primarily a guardian of the truth rather than primarily a seeker of it. ([Guardians Of The Truth](http://lesswrong.com/lw/lz/guardians_of_the_truth/)) The Nazis can be understood as would-be guardians of the gene pool. ([Guardians Of The Gene Pool](http://lesswrong.com/lw/m0/guardians_of_the_gene_pool/))

There are things we know now which earlier generations could not have known, which means that from our perspective we should expect elementary errors even in our historic geniuses. This is a defining attribute of scientific disciplines. It feels unfair to consider things they could not have known to be flaws in their ideas, but nevertheless they are. It is foolish to declare a system of ideas to be closed to further development. We already have examples of people who declared themselves to be about being Rational who fell into that trap in history. ([Guardians Of Ayn Rand](http://lesswrong.com/lw/m1/guardians_of_ayn_rand/))

Two ideas for countering a tendency towards affective death spirals around a group are to prefer using and describing techniques over citing authority, and to deliberately look foolish to reduce the positive affect you give to the techniques you describe, so they are judged on their own merits. ([Two Cult Koans](http://lesswrong.com/lw/m4/two_cult_koans/))

We tend to conform to the beliefs of those around us, and are especially inclined to avoid being the first dissenter, for social reasons. Being the first dissenter is thus a valuable service. ([Asch's Conformity Experiment](http://lesswrong.com/lw/m9/aschs_conformity_experiment/)) It can be correct if you do not believe you have any special advantage to believe that the majority opinion is more likely to be the true one, but it remains important to express your concerns. Doing so is generally just as socially discouraged as outright disagreement. ([On Expressing Your Concerns](http://lesswrong.com/lw/ma/on_expressing_your_concerns/))

Lonely dissent is often just a role people play in defined patterns. When it is real, it requires bearing the incomprehension of the people around you and discussing ideas that are not forbidden but outside bounds which aren’t even thought about. Doing this without a single other person is terrifying. Being different for its own sake is a bias like any other. ([Lonely Dissent](http://lesswrong.com/lw/mb/lonely_dissent/))

Cults vary from sincere but deluded and expensive groups, to “love bombing”, sleep deprivation, induced fatigue, distant communes, and daily meetings to confess impure thoughts. Lists of cult characteristics include things which describe other organisations, like political parties and corporations. The true defining aspect is the affective death spiral, which should be fought in any group, and judged independently of how weird the group is in other respects. ([Cultish Countercultishness](http://lesswrong.com/lw/md/cultish_countercultishness/))

### How To Actually Change Your Mind K: Letting Go

If we only admit small, local errors, we only make small, local improvements. Big improvements require admitting big errors. Rather than grudgingly admitting the smallest errors possible, be willing to consider that you may have made fundamental mistakes. ([The Importance Of Saying "Oops"](http://lesswrong.com/lw/i9/the_importance_of_saying_oops/))

Reinterpreting your mistakes to make it so that you were right ‘deep down’, or morally right, or half-right, avoids the opportunity to see large errors in the route you are on and adjust. ([The Crackpot Offer](http://lesswrong.com/lw/j8/the_crackpot_offer/)) Being ready to admit you lost lets you avoid turning small mistakes into bigger ones. ([Just Lose Hope Already](http://lesswrong.com/lw/gx/just_lose_hope_already/))

A doubt exists to potentially destroy a particular belief, on the basis of some specific justification. A doubt that fails to either be destroyed or destroy its belief may as well not have existed at all. Wearing doubts as attire does not make you more rational. ([The Proper Use Of Doubt](http://lesswrong.com/lw/ib/the_proper_use_of_doubt/))

You can face reality. _What is true is already so. Owning up to it doesn’t make it any worse._ ([You Can Face Reality](http://lesswrong.com/lw/id/you_can_face_reality/))

Criticising yourself from a sense of duty leaves you wanting to have investigated, not wanting to investigate. This leads to motivated stopping. There is no substitute for genuine curiosity, so attempt to cultivate it. Conservation of expected evidence means any process you think may confirm your beliefs you must also think may disconfirm them. If you do not, ask whether you are looking at only the strong points of your belief. ([The Meditation On Curiosity](http://lesswrong.com/lw/jz/the_meditation_on_curiosity/))

The laws governing evidence and belief are not social, but aspects of reality. They are not created by rationalists, but merely guessed at. No one can excuse you from them, any more than they may excuse you from the laws of gravity, regardless of how unfair they are in either case. ([No One Can Exempt You From Rationality's Laws](http://lesswrong.com/lw/k1/no_one_can_exempt_you_from_rationalitys_laws/))

When you have a cherished belief, ask yourself what you would do, assuming that it was false. Visualise the world in which it is false, without challenging that assumption. Answering this grants yourself a _line of retreat_\- a calm, tolerable path forward- enabling you to consider the question. ([Leave A Line Of Retreat](http://lesswrong.com/lw/o4/leave_a_line_of_retreat/))

When you are invested heavily and emotionally in a long-lived belief which is surrounded by arguments and refutations, it can be desirable to attempt to instigate a real crisis of faith about it, one that could go either way, as it will take more than an ordinary effort to displace if false. ([Crisis Of Faith](http://lesswrong.com/lw/ur/crisis_of_faith/), [The Ritual](http://lesswrong.com/lw/us/the_ritual/))

[(Continue with "The Machine In The Ghost")](/summaries/a-whirlwind-tour-of-lw-rationality-the-machine-in-the-ghost)
